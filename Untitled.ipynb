{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2596</th>\n",
       "      <th>51</th>\n",
       "      <th>3</th>\n",
       "      <th>258</th>\n",
       "      <th>0</th>\n",
       "      <th>510</th>\n",
       "      <th>221</th>\n",
       "      <th>232</th>\n",
       "      <th>148</th>\n",
       "      <th>6279</th>\n",
       "      <th>...</th>\n",
       "      <th>0.34</th>\n",
       "      <th>0.35</th>\n",
       "      <th>0.36</th>\n",
       "      <th>0.37</th>\n",
       "      <th>0.38</th>\n",
       "      <th>0.39</th>\n",
       "      <th>0.40</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2579</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>-15</td>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>6031</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2596   51   3  258    0   510  221  232  148  6279  ...  0.34  0.35  0.36  \\\n",
       "0  2590   56   2  212   -6   390  220  235  151  6225  ...     0     0     0   \n",
       "1  2804  139   9  268   65  3180  234  238  135  6121  ...     0     0     0   \n",
       "2  2785  155  18  242  118  3090  238  238  122  6211  ...     0     0     0   \n",
       "3  2595   45   2  153   -1   391  220  234  150  6172  ...     0     0     0   \n",
       "4  2579  132   6  300  -15    67  230  237  140  6031  ...     0     0     0   \n",
       "\n",
       "   0.37  0.38  0.39  0.40  0.41  0.42  5  \n",
       "0     0     0     0     0     0     0  5  \n",
       "1     0     0     0     0     0     0  2  \n",
       "2     0     0     0     0     0     0  2  \n",
       "3     0     0     0     0     0     0  5  \n",
       "4     0     0     0     0     0     0  2  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "import pandas as pd\n",
    "task1=pd.read_csv(\"covtype.csv\")\n",
    "task1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas\n",
    "dataset = task1.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "rem = []\n",
    "\n",
    "#Add constant columns as they don't help in prediction process\n",
    "for c in dataset.columns:\n",
    "    if dataset[c].std() == 0: #standard deviation is zero\n",
    "        rem.append(c)\n",
    "\n",
    "#drop the columns        \n",
    "dataset.drop(rem,axis=1,inplace=True)\n",
    "\n",
    "print(rem)\n",
    "\n",
    "#Following columns are dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = dataset.shape\n",
    "\n",
    "#get the list of columns\n",
    "cols = dataset.columns\n",
    "#create an array which has indexes of columns\n",
    "i_cols = []\n",
    "for i in range(0,c-1):\n",
    "    i_cols.append(i)\n",
    "#array of importance rank of all features  \n",
    "ranks = []\n",
    "\n",
    "#Extract only the values\n",
    "array = dataset.values\n",
    "\n",
    "#Y is the target column, X has the rest\n",
    "X_orig = array[:,0:(c-1)]\n",
    "Y = array[:,(c-1)]\n",
    "\n",
    "#Validation chunk size\n",
    "val_size = 0.3\n",
    "\n",
    "#Use a common seed in all experiments so that same chunk is used for validation\n",
    "seed = 0\n",
    "\n",
    "#Split the data into chunks\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val =train_test_split(X_orig, Y, test_size=val_size, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#All features\n",
    "X_all = []\n",
    "#Additionally we will make a list of subsets\n",
    "X_all_add =[]\n",
    "\n",
    "#columns to be dropped\n",
    "rem_cols = []\n",
    "#indexes of columns to be dropped\n",
    "i_rem = []\n",
    "\n",
    "#Add this version of X to the list \n",
    "X_all.append(['Orig','All', X_train,X_val,1.0,cols[:c-1],rem_cols,ranks,i_cols,i_rem])\n",
    "\n",
    "#point where categorical data begins\n",
    "size=10\n",
    "\n",
    "import numpy\n",
    "\n",
    "#Standardized\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = StandardScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['StdSca','All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n",
    "\n",
    "#MinMax\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['MinMax', 'All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n",
    "\n",
    "#Normalize\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = Normalizer().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['Norm', 'All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n",
    "\n",
    "#Impute\n",
    "#Imputer is not used as no data is missing\n",
    "\n",
    "#List of transformations\n",
    "trans_list = []\n",
    "\n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "    trans_list.append(trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list = [0.75,0.50,0.25]\n",
    "\n",
    "#Median of rankings for each column\n",
    "unsorted_rank = [0,8,11,4,5,2,5,7.5,9.5,3,8,28.5,14.5,2,35,19.5,12,14,37,25.5,50,44,9,28,20.5,19.5,40,38,20,38,43,35,44,22,24,33,49,42,46,47,27.5,19,31.5,23,28,42,30.5,46,40,12,13,18]\n",
    "\n",
    "#List of feature selection models\n",
    "feat = []\n",
    "\n",
    "#Add Median to the list \n",
    "n = 'Median'\n",
    "for val in ratio_list:\n",
    "    feat.append([n,val])   \n",
    "\n",
    "for trans,s, X, X_val, d, cols, rem_cols, ra, i_cols, i_rem in X_all:\n",
    "    #Create subsets of feature list based on ranking and ratio_list\n",
    "    for name, v in feat:\n",
    "        #Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        for i, pred in enumerate(unsorted_rank):\n",
    "            joined.append([i,cols[i],pred])\n",
    "        #Sort in descending order    \n",
    "        joined_sorted = sorted(joined, key=lambda x: x[2])\n",
    "        #Starting point of the columns to be dropped\n",
    "        rem_start = int((v*(c-1)))\n",
    "        #List of names of columns selected\n",
    "        cols_list = []\n",
    "        #Indexes of columns selected\n",
    "        i_cols_list = []\n",
    "        #Ranking of all the columns\n",
    "        rank_list =[]\n",
    "        #List of columns not selected\n",
    "        rem_list = []\n",
    "        #Indexes of columns not selected\n",
    "        i_rem_list = []\n",
    "        #Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            #Store the rank\n",
    "            rank_list.append([i,j])\n",
    "            #Store selected columns in cols_list and indexes in i_cols_list\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            #Store not selected columns in rem_list and indexes in i_rem_list    \n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)    \n",
    "        #Sort the rank_list and store only the ranks. Drop the index \n",
    "        #Append model name, array, columns selected and columns to be removed to the additional list        \n",
    "        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt    \n",
    "\n",
    "#Dictionary to store the accuracies for all combinations \n",
    "acc = {}\n",
    "\n",
    "#List of combinations\n",
    "comb = []\n",
    "\n",
    "#Append name of transformation to trans_list\n",
    "for trans in trans_list:\n",
    "    acc[trans]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Set the base model\n",
    "model = GaussianNB()\n",
    "algo = \"NB\"\n",
    "\n",
    "##Set figure size\n",
    "#plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "#Accuracy of the model using all features\n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "    model.fit(X[:,i_cols_list],Y_train)\n",
    "    result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "    acc[trans].append(result)\n",
    "    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "    #print(result)\n",
    "comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n",
    "        \n",
    "#Accuracy of the model using a subset of features    \n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "    model.fit(X[:,i_cols_list],Y_train)\n",
    "    result = model.score(X_val[:,i_cols_list], Y_val)\n",
    "    acc[trans].append(result)\n",
    "    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n",
    "    #print(result)\n",
    "for v in ratio_list:\n",
    "    comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n",
    "    \n",
    "##Plot the accuracies of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "##Plot each transformation\n",
    "#for trans in trans_list:\n",
    "#        plt.plot(acc[trans])\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Display the plot\n",
    "#plt.legend(trans_list,loc='best')    \n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#Best estimated performance is close to 64%. Original with 50% subset outperfoms all transformations of NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "n_estimators = 100\n",
    "\n",
    "#Obtain the list of indexes for the required model\n",
    "indexes = []\n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "    if v == 0.5:\n",
    "        if trans == 'Orig':\n",
    "            indexes = i_cols_list\n",
    "            break\n",
    "\n",
    "#Best model definition\n",
    "best_model = ExtraTreesClassifier(n_jobs=-1,n_estimators=n_estimators)\n",
    "best_model.fit(X_orig[:,indexes],Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "....\n",
    "\n",
    "\n",
    "\n",
    "#second Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2596</th>\n",
       "      <th>51</th>\n",
       "      <th>3</th>\n",
       "      <th>258</th>\n",
       "      <th>0</th>\n",
       "      <th>510</th>\n",
       "      <th>221</th>\n",
       "      <th>232</th>\n",
       "      <th>148</th>\n",
       "      <th>6279</th>\n",
       "      <th>...</th>\n",
       "      <th>0.34</th>\n",
       "      <th>0.35</th>\n",
       "      <th>0.36</th>\n",
       "      <th>0.37</th>\n",
       "      <th>0.38</th>\n",
       "      <th>0.39</th>\n",
       "      <th>0.40</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2596   51  3  258   0   510  221  232  148  6279  ...  0.34  0.35  0.36  \\\n",
       "0  2590   56  2  212  -6   390  220  235  151  6225  ...     0     0     0   \n",
       "1  2804  139  9  268  65  3180  234  238  135  6121  ...     0     0     0   \n",
       "\n",
       "   0.37  0.38  0.39  0.40  0.41  0.42  5  \n",
       "0     0     0     0     0     0     0  5  \n",
       "1     0     0     0     0     0     0  2  \n",
       "\n",
       "[2 rows x 55 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv(\"covtype.csv\")\n",
    "\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [48.62592016963651, 48.95242225997277, 48.75603908386646, 48.669293141046495, 48.698208455319815, 48.6271249743979, 48.987361598053035]\n",
      "Mean Accuracy: 48.759%\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from math import sqrt\n",
    "from math import exp\n",
    "from math import pi\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor _ in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "# Split the dataset by class values, returns a dictionary\n",
    "def separate_by_class(dataset):\n",
    "\tseparated = dict()\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tvector = dataset[i]\n",
    "\t\tclass_value = vector[-1]\n",
    "\t\tif (class_value not in separated):\n",
    "\t\t\tseparated[class_value] = list()\n",
    "\t\tseparated[class_value].append(vector)\n",
    "\treturn separated\n",
    " \n",
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "\treturn sum(numbers)/float(len(numbers))\n",
    " \n",
    "# Calculate the standard deviation of a list of numbers\n",
    "def stdev(numbers):\n",
    "\tavg = mean(numbers)\n",
    "\tvariance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "\treturn sqrt(variance)\n",
    " \n",
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "def summarize_dataset(dataset):\n",
    "\tsummaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "\tdel(summaries[-1])\n",
    "\treturn summaries\n",
    " \n",
    "# Split dataset by class then calculate statistics for each row\n",
    "def summarize_by_class(dataset):\n",
    "\tseparated = separate_by_class(dataset)\n",
    "\tsummaries = dict()\n",
    "\tfor class_value, rows in separated.items():\n",
    "\t\tsummaries[class_value] = summarize_dataset(rows)\n",
    "\treturn summaries\n",
    " \n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "#def calculate_probability(x, mean, stdev):\n",
    "#\texponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "#\treturn (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    " \n",
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "\ttotal_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "\tprobabilities = dict()\n",
    "\tfor class_value, class_summaries in summaries.items():\n",
    "\t\tprobabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "\t\tfor i in range(len(class_summaries)):\n",
    "\t\t\tmean, stdev, _ = class_summaries[i]\n",
    "#\t\t\tprobabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "\treturn probabilities\n",
    " \n",
    "# Predict the class for a given row\n",
    "def predict(summaries, row):\n",
    "\tprobabilities = calculate_class_probabilities(summaries, row)\n",
    "\tbest_label, best_prob = None, -1\n",
    "\tfor class_value, probability in probabilities.items():\n",
    "\t\tif best_label is None or probability > best_prob:\n",
    "\t\t\tbest_prob = probability\n",
    "\t\t\tbest_label = class_value\n",
    "\treturn best_label\n",
    " \n",
    "# Naive Bayes Algorithm\n",
    "def naive_bayes(train, test):\n",
    "\tsummarize = summarize_by_class(train)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\toutput = predict(summarize, row)\n",
    "\t\tpredictions.append(output)\n",
    "\treturn(predictions)\n",
    " \n",
    "# Test Naive Bayes on Iris Dataset\n",
    "seed(1)\n",
    "filename = 'covtype.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 7\n",
    "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = summarize_by_class(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\t\tprint('[%s] => %d' % (value, i))\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] => 0\n",
      "[4] => 1\n",
      "[1] => 2\n",
      "[6] => 3\n",
      "[3] => 4\n",
      "[5] => 5\n",
      "[7] => 6\n",
      "Data=[5.7, 2.9, 4.2, 1.3, 2, 4, 1.5], Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "from math import sqrt\n",
    "from math import exp\n",
    "from math import pi\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\t\tprint('[%s] => %d' % (value, i))\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Split the dataset by class values, returns a dictionary\n",
    "def separate_by_class(dataset):\n",
    "\tseparated = dict()\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tvector = dataset[i]\n",
    "\t\tclass_value = vector[-1]\n",
    "\t\tif (class_value not in separated):\n",
    "\t\t\tseparated[class_value] = list()\n",
    "\t\tseparated[class_value].append(vector)\n",
    "\treturn separated\n",
    " \n",
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "\treturn sum(numbers)/float(len(numbers))\n",
    " \n",
    "# Calculate the standard deviation of a list of numbers\n",
    "def stdev(numbers):\n",
    "\tavg = mean(numbers)\n",
    "\tvariance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "\treturn sqrt(variance)\n",
    " \n",
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "def summarize_dataset(dataset):\n",
    "\tsummaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "\tdel(summaries[-1])\n",
    "\treturn summaries\n",
    " \n",
    "# Split dataset by class then calculate statistics for each row\n",
    "def summarize_by_class(dataset):\n",
    "\tseparated = separate_by_class(dataset)\n",
    "\tsummaries = dict()\n",
    "\tfor class_value, rows in separated.items():\n",
    "\t\tsummaries[class_value] = summarize_dataset(rows)\n",
    "\treturn summaries\n",
    " \n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "\texponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "\treturn (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    " \n",
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "\ttotal_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "\tprobabilities = dict()\n",
    "\tfor class_value, class_summaries in summaries.items():\n",
    "\t\tprobabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "\t\tfor i in range(len(class_summaries)):\n",
    "\t\t\tmean, stdev, _ = class_summaries[i]\n",
    "#\t\t\tprobabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "\treturn probabilities\n",
    " \n",
    "# Predict the class for a given row\n",
    "def predict(summaries, row):\n",
    "\tprobabilities = calculate_class_probabilities(summaries, row)\n",
    "\tbest_label, best_prob = None, -1\n",
    "\tfor class_value, probability in probabilities.items():\n",
    "\t\tif best_label is None or probability > best_prob:\n",
    "\t\t\tbest_prob = probability\n",
    "\t\t\tbest_label = class_value\n",
    "\treturn best_label\n",
    " \n",
    "# Make a prediction with Naive Bayes on Iris Dataset\n",
    "filename = 'covtype.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# fit model\n",
    "model = summarize_by_class(dataset)\n",
    "# define a new record\n",
    "row = [5.7,2.9,4.2,1.3,2,4,1.5]\n",
    "# predict the label\n",
    "label = predict(model, row)\n",
    "print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
